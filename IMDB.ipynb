{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data Processing\nimport numpy as np\nimport pandas as pd\nimport re\n\nimport os\n\n# Word2Vec Library\nfrom nltk.tokenize import word_tokenize\n\n# Graphing Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-09T12:35:39.650491Z","iopub.execute_input":"2026-02-09T12:35:39.650860Z","iopub.status.idle":"2026-02-09T12:35:39.663132Z","shell.execute_reply.started":"2026-02-09T12:35:39.650806Z","shell.execute_reply":"2026-02-09T12:35:39.660600Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Dataset Cleaning\n\nWe first need to remove the html that has been left in the dataset, as well as changing the representation of positive/negative to 1/0.","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n\ndf = pd.read_csv(path)\ndf['sentiment'] = df['sentiment'].replace('positive', 1).replace('negative', 0)\n\nremove_html = lambda x : re.sub(r'<.*?>', '', x).lower()\ndf['review'] = df['review'].str.strip()\ndf['review'] = df['review'].apply(remove_html)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T12:36:47.533751Z","iopub.execute_input":"2026-02-09T12:36:47.534603Z","iopub.status.idle":"2026-02-09T12:36:49.243173Z","shell.execute_reply.started":"2026-02-09T12:36:47.534559Z","shell.execute_reply":"2026-02-09T12:36:49.242318Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_54/27206245.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df['sentiment'] = df['sentiment'].replace('positive', 1).replace('negative', 0)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              review  sentiment\n0  one of the other reviewers has mentioned that ...          1\n1  a wonderful little production. the filming tec...          1\n2  i thought this was a wonderful way to spend ti...          1\n3  basically there's a family where a little boy ...          0\n4  petter mattei's \"love in the time of money\" is...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a wonderful little production. the filming tec...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically there's a family where a little boy ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei's \"love in the time of money\" is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"This class will allow us to batch our data for more efficient model training.","metadata":{}},{"cell_type":"code","source":"memo = {}\nvocab_size = 0\ndef sentence_to_number(token_sentence):\n    # algorithm to one-hot-encode all tokens\n    global vocab_size\n    output = []\n    for word in token_sentence:\n        if word in memo.keys():\n            output.append(memo[word])\n        else:\n            vocab_size = vocab_size + 1\n            memo[word] = vocab_size\n            output.append(vocab_size)\n    return output\n\ndf['tokenizedReview'] = df['review'].apply(word_tokenize)\ndf['tokenizedReview'] = df['tokenizedReview'].apply(sentence_to_number)\ndf.to_csv('/kaggle/working/cleaned_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T12:36:54.623136Z","iopub.execute_input":"2026-02-09T12:36:54.624019Z","iopub.status.idle":"2026-02-09T12:38:03.931157Z","shell.execute_reply.started":"2026-02-09T12:36:54.623982Z","shell.execute_reply":"2026-02-09T12:38:03.929167Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# LSTM Network\n\nWe shall attempt to create a LSTM neural network to classify our IMDB reviews. These variables should be useful for us.","metadata":{}},{"cell_type":"markdown","source":"We introduce some variables to help fine-tune our model further down the line.","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-3\nweight_decay = 1e-5\nhidden_size = 256\nepochs = 100\nbatch_size = 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:09:18.289871Z","iopub.execute_input":"2026-02-08T18:09:18.290119Z","iopub.status.idle":"2026-02-08T18:09:18.293706Z","shell.execute_reply.started":"2026-02-08T18:09:18.290091Z","shell.execute_reply":"2026-02-08T18:09:18.292986Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Reviews(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.reviews = df['tokenizedReview'].tolist()\n        self.labels = df['sentiment'].tolist()\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, i):\n        return self.reviews[i], float(self.labels[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:09:18.294724Z","iopub.execute_input":"2026-02-08T18:09:18.295010Z","iopub.status.idle":"2026-02-08T18:09:18.307112Z","shell.execute_reply.started":"2026-02-08T18:09:18.294980Z","shell.execute_reply":"2026-02-08T18:09:18.306395Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Now, the idea is to feed through each token into our RNN and minimise the loss between the RNN output and the sentiment column. We should split our data into training and testing sets to work with.","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    # Pads each batch with zeros\n    reviews, labels = zip(*batch)\n\n    n = max([len(review) for review in reviews])\n    padded = [review + [0] * (n-len(review)) for review in reviews]\n\n    return torch.tensor(padded, dtype=torch.long), torch.tensor(labels).unsqueeze(1)\n\ntrain, test = train_test_split(df, test_size=0.2)\ntest, validation = train_test_split(test, train_size=0.8)\nprint('''\ntrain length : {}\ntest length : {}\nvalidation length : {}\n'''.format(len(train), len(test), len(validation)))\n\ntrain_set = Reviews(train)\ntest_set = Reviews(test)\nval_set = Reviews(validation)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_set, batch_size=1, shuffle=True, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:09:18.307923Z","iopub.execute_input":"2026-02-08T18:09:18.308153Z","iopub.status.idle":"2026-02-08T18:09:18.341210Z","shell.execute_reply.started":"2026-02-08T18:09:18.308124Z","shell.execute_reply":"2026-02-08T18:09:18.340545Z"}},"outputs":[{"name":"stdout","text":"\ntrain length : 40000\ntest length : 8000\nvalidation length : 2000\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Here we create a basic Recurrent Neural Network, allowing us to feed previous \"words\"(tokens) in the sentence into our function. This allows us to provide context to words, hopefully leading to a more accurate model.","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n\n    def __init__(self, vocab_size, hidden_size, embedded_size=hidden_size):\n        self.hidden_size = hidden_size\n        super(LSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedded_size, padding_idx=0)\n        self.model_y = nn.Linear(self.hidden_size, 1)\n        self.dropout = nn.Dropout(p=0.2)\n        \n        self.forget = nn.Linear(embedded_size+self.hidden_size, self.hidden_size)\n        self.input_gate = nn.Linear(embedded_size+self.hidden_size, self.hidden_size)\n        self.candidate = nn.Linear(embedded_size+self.hidden_size, self.hidden_size)\n        self.output_gate = nn.Linear(embedded_size+self.hidden_size, self.hidden_size)\n        \n        self.tanh = nn.Tanh()\n        \n        nn.init.constant_(self.forget.bias, 1.0)\n        \n    def forward(self, tokens):\n        batch_size, seq_length = tokens.shape\n        h = torch.zeros(batch_size, self.hidden_size, device=device)\n        c = torch.zeros(batch_size, self.hidden_size, device=device)\n        for i in range(0, seq_length):\n            x = tokens[:, i]\n            mask = (x != 0).unsqueeze(1).float()\n            h_new, c_new = self.lstm_block(x, h, c)\n            h_new, c_new = self.lstm_block(x, h, c)\n            h = mask * h_new + (1 - mask) * h\n            c = mask * c_new + (1 - mask) * c\n        h = self.dropout(h)\n        y = self.model_y(h)\n        return y\n\n    def lstm_block(self, x, hprev, cprev):\n        embedded = torch.cat([self.embedding(x), hprev], dim=1)\n        f = torch.sigmoid(self.forget(embedded))\n        i = torch.sigmoid(self.input_gate(embedded))\n        candidate_c = self.tanh(self.candidate(embedded))    \n        c = torch.mul(f, cprev) + torch.mul(i, candidate_c)\n        output = torch.sigmoid(self.output_gate(embedded))\n        h = torch.mul(output, self.tanh(c))\n        return h, c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:09:18.342103Z","iopub.execute_input":"2026-02-08T18:09:18.342338Z","iopub.status.idle":"2026-02-08T18:09:18.352379Z","shell.execute_reply.started":"2026-02-08T18:09:18.342317Z","shell.execute_reply":"2026-02-08T18:09:18.351720Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Now we need to train our model. To do this, we must loop through each review until the end, and then use backpropogation to update our values. We shall use a BCE loss function, as this is appropriate for a yes / no classification problem. ","metadata":{}},{"cell_type":"code","source":"model = LSTM(max(memo.values()) + 1, hidden_size)\noptim = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nloss_fn = nn.BCEWithLogitsLoss()\n\ntrain_losses = []\nval_losses = []\n\nbest_val = 1 # average validation loss will always be less than one for binary classification\ndelta = 1e-3\ncount = 0\npatience = 2\n\nprint('TRAINING STARTED | DEVICE : {}'.format(device))\nprint('-'*20)\nmodel.to(device)\nfor epoch in range(epochs):\n    running_loss = 0\n    model.train()\n    for tokens, labels in train_loader:\n        tokens = tokens.to(device)\n        labels = labels.to(device)\n\n        y = model(tokens)\n        loss = loss_fn(y, labels)\n        \n        train_losses.append(loss.item())\n        running_loss += loss.item()\n\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for tokens, labels in val_loader:\n            tokens = tokens.to(device)\n            labels = labels.to(device)\n            y = model(tokens)\n            loss = loss_fn(y, labels)\n            val_losses.append(loss.item())\n            val_loss += loss.item()\n        avg_val_loss = val_loss/len(val_loader)\n        print('EPOCH : {i:03d} | TRAINING LOSS : {:.4f} | VALIDATION LOSS : {:.4f}'.format(\n            epoch+1, \n            running_loss/len(train_loader), \n            avg_val_loss))\n    if avg_val_loss > best_val - delta:\n        count += 1\n    else:\n        count = 0\n        best_val = avg_val_loss\n        torch.save(model.state_dict(), '/kaggle/working/model_hidden_epoch_{}.pt'.format(epoch))\n    if count >= patience:\n        print('EARLY STOPPING')\n        break\n    \ntorch.save(model.state_dict(), '/kaggle/working/model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:09:18.353215Z","iopub.execute_input":"2026-02-08T18:09:18.353421Z"}},"outputs":[{"name":"stdout","text":"STARTING TRAINING SEQUENCE ON DEVICE : cuda\nEPOCH : 1.0000 | TRAINING LOSS : 0.5434 | VALIDATION LOSS : 0.5241863441825845\nEPOCH : 2.0000 | TRAINING LOSS : 0.3913 | VALIDATION LOSS : 0.2671877353384625\nEPOCH : 3.0000 | TRAINING LOSS : 0.2202 | VALIDATION LOSS : 0.24118825167880278\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation\n\nA good visualisation regarding whether our model has done well comes in the form of a confusion matrix. Of course we will also use Mathews Corellation Coefficient which should tell us if our model has performed well.","metadata":{}},{"cell_type":"code","source":"PATH = '/kaggle/working/model_epoch_4.pt'\nmodel = LSTM(max(memo.values()) + 1, hidden_size).to(device)\ntorch.load(PATH, weights_only=True)\nmodel.eval()\n\nactual = []\nprediction = []\nwith torch.no_grad():\n    for tokens, labels in test_loader:\n        tokens = tokens.to(device)\n        labels = labels.to(device)\n        batch_size, seq_len = tokens.shape\n        y = model(tokens)\n        for i in range(0, len(y)):\n            prediction.append(y[i].item())\n            actual.append(labels[i].item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 0.5\npreds = [0 if x < threshold else 1 for x in prediction]\ncm = confusion_matrix(actual, preds)\nConfusionMatrixDisplay(cm).plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To evaluate this model, we shall use Mathews Correlation Coefficient. The formula for which is as follows:\n$$\n\\frac{TP\\times TN - FP\\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n$$","metadata":{}},{"cell_type":"code","source":"precision = cm[1, 1]/(cm[0, 1] + cm[1, 1])\nrecall = cm[1,1]/(cm[1,0]+cm[1,1])\ntrue_positive = cm[1, 1]\ntrue_negative = cm[0, 0]\nfalse_positive = cm[0, 1]\nfalse_negative = cm[1, 0]\nmcc = (true_positive*true_negative-false_positive*false_negative)/(((true_positive+false_positive)(true_positive+false_negative)(true_negative+false_positive)(true_negative+false_negative))**0.5)\nf1 = 2*precision*recall/(precision+recall)\nprint('The F1 Score is {}'.format(f1))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}